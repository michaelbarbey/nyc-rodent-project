{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8010195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl             # faster and more efficient than pandas\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "# base_url = 'https://data.cityofnewyork.us/api/v3/views/p937-wjvj/query.csv'\n",
    "block_size = 100_000  # Number of rows per block\n",
    "#output_parquet = 'rodent_inspection_data.parquet'\n",
    "output_parquet = '311_service_requests_data.parquet'\n",
    "\n",
    "base_url = 'https://data.cityofnewyork.us/resource/erm2-nwe9.csv'\n",
    "max_workers = 4 # Number of threads for parallel processing\n",
    "\n",
    "columns = [ # channel_type --> open_data_channel_type\n",
    "    \"unique_key\", \"created_date\", \"closed_date\", \"agency\", \"agency_name\",\n",
    "    \"complaint_type\", \"descriptor\", \"location_type\", \"incident_zip\",\n",
    "    \"city\", \"status\", \"resolution_action_updated_date\", \"borough\", \"open_data_channel_type\"\n",
    "]\n",
    "\n",
    "def download_block (offset):\n",
    "    # Socrata API in SQL -- pending to test SODA 3\n",
    "    soql = (\n",
    "        f\"SELECT {', '.join(columns)} \"\n",
    "        f\"LIMIT {block_size} OFFSET {offset}\"  \n",
    "    )\n",
    "    \n",
    "    encoded_query = urllib.parse.quote(soql, safe='')\n",
    "    url = f\"{base_url}?$query={encoded_query}\"\n",
    "    \n",
    "    try: \n",
    "        df_block = pl.read_csv(url, \n",
    "                               columns=columns, \n",
    "                               schema_overrides={\"incident_zip\": pl.Utf8}\n",
    "                               )\n",
    "        if df_block.height == 0:\n",
    "            return None\n",
    "        table = df_block.to_arrow() # File type transformation to arrow\n",
    "        return (offset, table)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading block at offset {offset}: {e}\")\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b9055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download loop and offset counter\n",
    "\n",
    "offset = 0\n",
    "first_block = True\n",
    "writer = None\n",
    "\n",
    "while True:\n",
    "    # Use ThreadPoolExecutor to download multiple blocks in parallel\n",
    "    offsets = [offset + i*block_size for i in range(max_workers)]\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(download_block, o): o for o in offsets}\n",
    "        \n",
    "        all_done = False\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is None:\n",
    "                all_done = True\n",
    "                continue\n",
    "            \n",
    "            block_offset, table = result\n",
    "            \n",
    "            # Initialize ParquetWriter on first block\n",
    "            if first_block:\n",
    "                writer = pq.ParquetWriter('311_service_requests_data.parquet', table.schema, compression='snappy')\n",
    "                first_block = False\n",
    "            \n",
    "            writer.write_table(table)\n",
    "            print(f\"Downloaded and wrote block at offset {block_offset}\")\n",
    "    if all_done:\n",
    "        break\n",
    "    offset += max_workers * block_size\n",
    "    \n",
    "# Closing file writer\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "print(\"All data downloaded and saved to Parquet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba9c06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building dimensional models --> modified application, create single build\n",
    "\n",
    "import urllib.parse\n",
    "base_url = 'https://data.cityofnewyork.us/resource/p937-wjvj.csv'\n",
    "block_size = 100_000  # Number of rows per block\n",
    "output_parquet = 'rodent_inspection_data.parquet'\n",
    "#output_parquet = '311_service_requests_data.parquet'\n",
    "\n",
    "#base_url = 'https://data.cityofnewyork.us/resource/erm2-nwe9.csv'\n",
    "max_workers = 4 # Number of threads for parallel processing\n",
    "\n",
    "columns = [ # inspection dimension\n",
    "    \"job_ticket_or_work_order_id\", \"job_id\", \"job_progress\", \"inspection_date\", \"result\",\n",
    "    \"borough\", \"inspection_type\", \"zip_code\" # added attributes for analysis\n",
    "]\n",
    "\n",
    "def download_block (offset):\n",
    "    # Socrata API in SQL -- pending to test SODA 3\n",
    "    soql = (\n",
    "        f\"SELECT {', '.join(columns)} \"\n",
    "        f\"LIMIT {block_size} OFFSET {offset}\"  \n",
    "    )\n",
    "    \n",
    "    encoded_query = urllib.parse.quote(soql, safe='')\n",
    "    url = f\"{base_url}?$query={encoded_query}\"\n",
    "    \n",
    "    try: \n",
    "        df_block = pl.read_csv(url, \n",
    "                               columns=columns, \n",
    "                               schema_overrides={\"zip_code\": pl.Utf8}\n",
    "                               )\n",
    "        if df_block.height == 0:\n",
    "            return None\n",
    "        table = df_block.to_arrow() # File type transformation to arrow\n",
    "        return (offset, table)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading block at offset {offset}: {e}\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4fe9e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and wrote block at offset 300000\n",
      "Downloaded and wrote block at offset 200000\n",
      "Downloaded and wrote block at offset 100000\n",
      "Downloaded and wrote block at offset 0\n",
      "Downloaded and wrote block at offset 700000\n",
      "Downloaded and wrote block at offset 400000\n",
      "Downloaded and wrote block at offset 600000\n",
      "Downloaded and wrote block at offset 500000\n",
      "Downloaded and wrote block at offset 1100000\n",
      "Downloaded and wrote block at offset 1000000\n",
      "Downloaded and wrote block at offset 800000\n",
      "Downloaded and wrote block at offset 900000\n",
      "Downloaded and wrote block at offset 1400000\n",
      "Downloaded and wrote block at offset 1200000\n",
      "Downloaded and wrote block at offset 1300000\n",
      "Downloaded and wrote block at offset 1500000\n",
      "Downloaded and wrote block at offset 1800000\n",
      "Downloaded and wrote block at offset 1700000\n",
      "Downloaded and wrote block at offset 1900000\n",
      "Downloaded and wrote block at offset 1600000\n",
      "Downloaded and wrote block at offset 2200000\n",
      "Downloaded and wrote block at offset 2100000\n",
      "Downloaded and wrote block at offset 2300000\n",
      "Downloaded and wrote block at offset 2000000\n",
      "Downloaded and wrote block at offset 2700000\n",
      "Downloaded and wrote block at offset 2400000\n",
      "Downloaded and wrote block at offset 2500000\n",
      "Downloaded and wrote block at offset 2600000\n",
      "Downloaded and wrote block at offset 2800000\n",
      "All data downloaded and saved to Parquet.\n"
     ]
    }
   ],
   "source": [
    "# download loop and offset counter\n",
    "\n",
    "offset = 0\n",
    "first_block = True\n",
    "writer = None\n",
    "\n",
    "while True:\n",
    "    # Use ThreadPoolExecutor to download multiple blocks in parallel\n",
    "    offsets = [offset + i*block_size for i in range(max_workers)]\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(download_block, o): o for o in offsets}\n",
    "        \n",
    "        all_done = False\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is None:\n",
    "                all_done = True\n",
    "                continue\n",
    "            \n",
    "            block_offset, table = result\n",
    "            \n",
    "            # Initialize ParquetWriter on first block\n",
    "            if first_block:\n",
    "                writer = pq.ParquetWriter('rodent_inspection_data.parquet', table.schema, compression='snappy')\n",
    "                first_block = False\n",
    "            \n",
    "            writer.write_table(table)\n",
    "            print(f\"Downloaded and wrote block at offset {block_offset}\")\n",
    "    if all_done:\n",
    "        break\n",
    "    offset += max_workers * block_size\n",
    "    \n",
    "# Closing file writer\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "print(\"All data downloaded and saved to Parquet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622af45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
