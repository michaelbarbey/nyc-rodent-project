{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8010195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl             # faster and more efficient than pandas\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff3c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "# base_url = 'https://data.cityofnewyork.us/api/v3/views/p937-wjvj/query.csv'\n",
    "block_size = 100_000  # Number of rows per block\n",
    "#output_parquet = 'rodent_inspection_data.parquet'\n",
    "output_parquet = '311_service_requests_data.parquet'\n",
    "\n",
    "base_url = 'https://data.cityofnewyork.us/resource/erm2-nwe9.csv'\n",
    "max_workers = 4 # Number of threads for parallel processing\n",
    "\n",
    "columns = [ # channel_type --> open_data_channel_type\n",
    "    \"unique_key\", \"created_date\", \"closed_date\", \"agency\", \"agency_name\",\n",
    "    \"complaint_type\", \"descriptor\", \"location_type\", \"incident_zip\",\n",
    "    \"city\", \"status\", \"resolution_action_updated_date\", \"borough\", \"open_data_channel_type\"\n",
    "]\n",
    "\n",
    "def download_block (offset):\n",
    "    # Socrata API in SQL -- pending to test SODA 3\n",
    "    soql = (\n",
    "        f\"SELECT {', '.join(columns)} \"\n",
    "        f\"LIMIT {block_size} OFFSET {offset}\"  \n",
    "    )\n",
    "    \n",
    "    encoded_query = urllib.parse.quote(soql, safe='')\n",
    "    url = f\"{base_url}?$query={encoded_query}\"\n",
    "    \n",
    "    try: \n",
    "        df_block = pl.read_csv(url, \n",
    "                               columns=columns, \n",
    "                               schema_overrides={\"incident_zip\": pl.Utf8}\n",
    "                               )\n",
    "        if df_block.height == 0:\n",
    "            return None\n",
    "        table = df_block.to_arrow() # File type transformation to arrow\n",
    "        return (offset, table)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading block at offset {offset}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b9055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download loop and offset counter\n",
    "\n",
    "offset = 0\n",
    "first_block = True\n",
    "writer = None\n",
    "\n",
    "while True:\n",
    "    # Use ThreadPoolExecutor to download multiple blocks in parallel\n",
    "    offsets = [offset + i*block_size for i in range(max_workers)]\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(download_block, o): o for o in offsets}\n",
    "        \n",
    "        all_done = False\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is None:\n",
    "                all_done = True\n",
    "                continue\n",
    "            \n",
    "            block_offset, table = result\n",
    "            \n",
    "            # Initialize ParquetWriter on first block\n",
    "            if first_block:\n",
    "                writer = pq.ParquetWriter('311_service_requests_data.parquet', table.schema, compression='snappy')\n",
    "                first_block = False\n",
    "            \n",
    "            writer.write_table(table)\n",
    "            print(f\"Downloaded and wrote block at offset {block_offset}\")\n",
    "    if all_done:\n",
    "        break\n",
    "    offset += max_workers * block_size\n",
    "    \n",
    "# Closing file writer\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "print(\"All data downloaded and saved to Parquet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9c06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building dimensional models --> modify application, create single build\n",
    "\n",
    "import urllib.parse\n",
    "base_url = 'https://data.cityofnewyork.us/resource/p937-wjvj.csv'\n",
    "block_size = 100_000  # Number of rows per block\n",
    "output_parquet = 'rodent_inspection_data.parquet'\n",
    "#output_parquet = '311_service_requests_data.parquet'\n",
    "\n",
    "#base_url = 'https://data.cityofnewyork.us/resource/erm2-nwe9.csv'\n",
    "max_workers = 4 # Number of threads for parallel processing\n",
    "\n",
    "columns = [ # inspection dimension\n",
    "    \"job_ticket_or_work_order_id\", \"job_id\", \"job_progress\", \"inspection_date\", \"result\",\n",
    "    \"borough\", \"inspection_type\", \"zip_code\", \"nta\" # added attributes for analysis\n",
    "]\n",
    "\n",
    "def download_block (offset):\n",
    "    # Socrata API in SQL -- pending to test SODA 3\n",
    "    soql = (\n",
    "        f\"SELECT {', '.join(columns)} \"\n",
    "        f\"LIMIT {block_size} OFFSET {offset}\"  \n",
    "    )\n",
    "    \n",
    "    encoded_query = urllib.parse.quote(soql, safe='')\n",
    "    url = f\"{base_url}?$query={encoded_query}\"\n",
    "    \n",
    "    try: \n",
    "        df_block = pl.read_csv(url, \n",
    "                               columns=columns, \n",
    "                               schema_overrides={\"zip_code\": pl.Utf8}\n",
    "                               )\n",
    "        if df_block.height == 0:\n",
    "            return None\n",
    "        table = df_block.to_arrow() # File type transformation to arrow\n",
    "        return (offset, table)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading block at offset {offset}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4fe9e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and wrote block at offset 200000\n",
      "Downloaded and wrote block at offset 0\n",
      "Downloaded and wrote block at offset 300000\n",
      "Downloaded and wrote block at offset 100000\n",
      "Downloaded and wrote block at offset 700000\n",
      "Downloaded and wrote block at offset 500000\n",
      "Downloaded and wrote block at offset 400000\n",
      "Downloaded and wrote block at offset 600000\n",
      "Downloaded and wrote block at offset 800000\n",
      "Downloaded and wrote block at offset 1000000\n",
      "Downloaded and wrote block at offset 1100000\n",
      "Downloaded and wrote block at offset 900000\n",
      "Downloaded and wrote block at offset 1400000\n",
      "Downloaded and wrote block at offset 1200000\n",
      "Downloaded and wrote block at offset 1300000\n",
      "Downloaded and wrote block at offset 1500000\n",
      "Downloaded and wrote block at offset 1600000\n",
      "Downloaded and wrote block at offset 1900000\n",
      "Downloaded and wrote block at offset 1800000\n",
      "Downloaded and wrote block at offset 1700000\n",
      "Downloaded and wrote block at offset 2000000\n",
      "Downloaded and wrote block at offset 2300000\n",
      "Downloaded and wrote block at offset 2100000\n",
      "Downloaded and wrote block at offset 2200000\n",
      "Downloaded and wrote block at offset 2400000\n",
      "Downloaded and wrote block at offset 2600000\n",
      "Downloaded and wrote block at offset 2500000\n",
      "Downloaded and wrote block at offset 2700000\n",
      "Downloaded and wrote block at offset 2800000\n",
      "All data downloaded and saved to Parquet.\n"
     ]
    }
   ],
   "source": [
    "# download loop and offset counter\n",
    "\n",
    "offset = 0\n",
    "first_block = True\n",
    "writer = None\n",
    "\n",
    "while True:\n",
    "    # Use ThreadPoolExecutor to download multiple blocks in parallel\n",
    "    offsets = [offset + i*block_size for i in range(max_workers)]\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(download_block, o): o for o in offsets}\n",
    "        \n",
    "        all_done = False\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is None:\n",
    "                all_done = True\n",
    "                continue\n",
    "            \n",
    "            block_offset, table = result\n",
    "            \n",
    "            # Initialize ParquetWriter on first block\n",
    "            if first_block:\n",
    "                writer = pq.ParquetWriter('rodent_inspection_data.parquet', table.schema, compression='snappy')\n",
    "                first_block = False\n",
    "            \n",
    "            writer.write_table(table)\n",
    "            print(f\"Downloaded and wrote block at offset {block_offset}\")\n",
    "    if all_done:\n",
    "        break\n",
    "    offset += max_workers * block_size\n",
    "    \n",
    "# Closing file writer\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "print(\"All data downloaded and saved to Parquet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d35f6b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "stg = pl.read_parquet(\"311_service_requests_data.parquet\")\n",
    "\n",
    "# converting created_date from string to datetime (ISO-ish: 2025-03-31T17:43:18.000)\n",
    "stg = stg.with_columns(\n",
    "    pl.col(\"created_date\").str.strptime(\n",
    "        pl.Datetime,\n",
    "        format=\"%Y-%m-%dT%H:%M:%S%.3f\",\n",
    "        strict=False   # empty or invalid values will return null\n",
    "    )\n",
    ")\n",
    "\n",
    "# converting closed_date and resolution_action_updated_date to datetime\n",
    "stg = stg.with_columns(\n",
    "    pl.col(\"closed_date\").str.strptime(\n",
    "        pl.Datetime,\n",
    "        format=\"%Y-%m-%dT%H:%M:%S%.3f\",\n",
    "        strict=False\n",
    "    ),\n",
    "    pl.col(\"resolution_action_updated_date\").str.strptime(\n",
    "        pl.Datetime,\n",
    "        format=\"%Y-%m-%dT%H:%M:%S%.3f\",\n",
    "        strict=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f622af45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_time\n",
      " status: created\n",
      "\n",
      "dim_agency\n",
      " status: created\n",
      "\n",
      "dim_status\n",
      " status: created\n",
      "\n",
      "dim_channel\n",
      " status: created\n",
      "\n",
      "dim_complaint\n",
      " status: created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# location dimension build | select query\n",
    "dim_location = (\n",
    "    stg.select([\n",
    "        pl.lit(\"NY\").alias(\"state\"),\n",
    "        pl.lit(\"USA\").alias(\"country\"),\n",
    "        \"city\",\n",
    "        \"borough\",\n",
    "        pl.col(\"incident_zip\").alias(\"zip_code\")\n",
    "    ]).unique().with_row_index(\"location_dim_id\") # surrogate key\n",
    ")\n",
    "# creating parquet file\n",
    "dim_location.write_parquet(\"dim_location.parquet\")\n",
    "\n",
    "# time dimension [ created_date ] \n",
    "dim_time = (\n",
    "    stg.select([\n",
    "        pl.col(\"created_date\").dt.date().alias(\"date\"),\n",
    "        pl.col(\"created_date\").dt.year().alias(\"year\"),\n",
    "        pl.col(\"created_date\").dt.month().alias(\"month\"),\n",
    "        pl.col(\"created_date\").dt.strftime(\"%Y-%m\").alias(\"YYYY-MM\"),\n",
    "        pl.col(\"created_date\").dt.strftime(\"%b\").alias(\"month_name\"), # abbreviated month name\n",
    "        pl.col(\"created_date\").dt.day().alias(\"day\"),\n",
    "        pl.col(\"created_date\").dt.week().alias(\"week\"),\n",
    "        pl.col(\"created_date\").dt.quarter().alias(\"quarter\"),\n",
    "    ]).unique().with_row_index(\"time_dim_id\")\n",
    ")\n",
    "# creating parquet file\n",
    "dim_time.write_parquet(\"dim_time.parquet\")\n",
    "print(\"dim_time\\n\",\n",
    "      \"status: created\\n\")\n",
    "\n",
    "# agency dimension\n",
    "dim_agency = (\n",
    "    stg.select([\n",
    "        \"agency\",\n",
    "        \"agency_name\"\n",
    "    ]).unique().with_row_index(\"agency_dim_id\")\n",
    ")\n",
    "# creating parquet file\n",
    "dim_agency.write_parquet(\"dim_agency.parquet\")\n",
    "print(\"dim_agency\\n\",\n",
    "      \"status: created\\n\")\n",
    "\n",
    "# status dimension\n",
    "dim_status = (\n",
    "    stg.select(pl.col(\"status\").alias(\"status_type\"))\n",
    "    .unique().with_row_index(\"status_dim_id\")\n",
    ")\n",
    "# creating parquet file\n",
    "dim_status.write_parquet(\"dim_status.parquet\")\n",
    "print(\"dim_status\\n\",\n",
    "      \"status: created\\n\")\n",
    "\n",
    "# channel type dimension\n",
    "dim_channel = (\n",
    "    stg.select(pl.col(\"open_data_channel_type\").alias(\"channel_type\"))\n",
    "    .unique().with_row_index(\"channel_dim_id\")\n",
    ")\n",
    "# creating parquet file\n",
    "dim_channel.write_parquet(\"dim_channel_type.parquet\")\n",
    "print(\"dim_channel\\n\",\n",
    "      \"status: created\\n\")\n",
    "\n",
    "# complaint type dimension\n",
    "dim_complaint = (\n",
    "    stg.select(pl.col(\"complaint_type\").alias(\"complaint_name\"))\n",
    "    .unique().with_row_index(\"complaint_type_dim_id\")\n",
    ")\n",
    "# creating parquet file\n",
    "dim_complaint.write_parquet(\"dim_complaint_type.parquet\")\n",
    "print(\"dim_complaint\\n\",\n",
    "      \"status: created\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a241b4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10, 1)\n",
      "┌─────────────────────┐\n",
      "│ created_date        │\n",
      "│ ---                 │\n",
      "│ datetime[ms]        │\n",
      "╞═════════════════════╡\n",
      "│ 2022-01-17 08:17:24 │\n",
      "│ 2022-01-17 07:00:12 │\n",
      "│ 2022-01-18 08:32:10 │\n",
      "│ 2022-01-17 07:14:00 │\n",
      "│ 2022-01-18 10:16:45 │\n",
      "│ 2022-01-18 14:20:16 │\n",
      "│ 2022-01-18 13:03:16 │\n",
      "│ 2022-01-19 00:47:42 │\n",
      "│ 2022-01-18 03:10:02 │\n",
      "│ 2022-01-18 11:30:24 │\n",
      "└─────────────────────┘\n",
      "Schema({'unique_key': Int64, 'created_date': Datetime(time_unit='ms', time_zone=None), 'closed_date': Datetime(time_unit='ms', time_zone=None), 'agency': String, 'agency_name': String, 'complaint_type': String, 'descriptor': String, 'location_type': String, 'incident_zip': String, 'city': String, 'status': String, 'resolution_action_updated_date': Datetime(time_unit='ms', time_zone=None), 'borough': String, 'open_data_channel_type': String})\n"
     ]
    }
   ],
   "source": [
    "print(stg.select(pl.col(\"created_date\").tail()))\n",
    "print(stg.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10b54047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating 311 service requests fact table\n",
    "fact = (\n",
    "    stg\n",
    "    # location join\n",
    "    .join(dim_location, left_on=[\"city\", \"borough\", \"incident_zip\"], \n",
    "                        right_on=[\"city\", \"borough\", \"zip_code\"], how=\"left\")\n",
    "    \n",
    "    # time join\n",
    "    .join(dim_time.select([\"time_dim_id\", \"date\"]), left_on=pl.col(\"created_date\").dt.date(), right_on=pl.col(\"date\"),how=\"left\")\n",
    "    \n",
    "    # agency join\n",
    "    .join(dim_agency, on=[\"agency\", \"agency_name\"], how=\"left\")\n",
    "    \n",
    "    # status join\n",
    "    .join(dim_status, left_on=\"status\", right_on=\"status_type\", how=\"left\")\n",
    "    \n",
    "    # channel type join\n",
    "    .join(dim_channel, left_on=\"open_data_channel_type\", right_on=\"channel_type\", how=\"left\")\n",
    "    \n",
    "    # complaint type join\n",
    "    .join(dim_complaint, left_on=\"complaint_type\", right_on=\"complaint_name\", how=\"left\")\n",
    "    \n",
    "    # aggregate to match the fact grain\n",
    "    .group_by([\n",
    "        \"location_dim_id\",\n",
    "        \"time_dim_id\",\n",
    "        \"channel_dim_id\",\n",
    "        \"complaint_type_dim_id\",\n",
    "        \"agency_dim_id\",\n",
    "        \"status_dim_id\"\n",
    "    ]).agg(\n",
    "        pl.count(\"unique_key\").alias(\"complaint_count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "fact.write_parquet(\"fact_311_service_requests.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc2e7805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 7)\n",
      "┌──────────────┬─────────────┬─────────────┬─────────────┬─────────────┬─────────────┬─────────────┐\n",
      "│ location_dim ┆ time_dim_id ┆ channel_dim ┆ complaint_t ┆ agency_dim_ ┆ status_dim_ ┆ complaint_c │\n",
      "│ _id          ┆ ---         ┆ _id         ┆ ype_dim_id  ┆ id          ┆ id          ┆ ount        │\n",
      "│ ---          ┆ u32         ┆ ---         ┆ ---         ┆ ---         ┆ ---         ┆ ---         │\n",
      "│ u32          ┆             ┆ u32         ┆ u32         ┆ u32         ┆ u32         ┆ u32         │\n",
      "╞══════════════╪═════════════╪═════════════╪═════════════╪═════════════╪═════════════╪═════════════╡\n",
      "│ 18           ┆ 4818        ┆ 3           ┆ 126         ┆ 11          ┆ 9           ┆ 3           │\n",
      "│ 1989         ┆ 2972        ┆ 0           ┆ 90          ┆ 624         ┆ 9           ┆ 1           │\n",
      "│ 2094         ┆ 3557        ┆ 0           ┆ 83          ┆ 159         ┆ 1           ┆ 1           │\n",
      "│ 2993         ┆ 233         ┆ 3           ┆ 123         ┆ 1039        ┆ 9           ┆ 1           │\n",
      "│ 1458         ┆ 3072        ┆ 2           ┆ 113         ┆ 624         ┆ 9           ┆ 6           │\n",
      "└──────────────┴─────────────┴─────────────┴─────────────┴─────────────┴─────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(fact.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10325dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating rodent inspection data dimensional model\n",
    "\n",
    "rodent_stg = pl.read_parquet(\"rodent_inspection_data.parquet\")\n",
    "\n",
    "# Convert created_date from string → datetime (ISO-ish: 2025-03-31T17:43:18.000)\n",
    "rodent_stg = rodent_stg.with_columns(\n",
    "    pl.col(\"inspection_date\").str.strptime(\n",
    "        pl.Datetime,\n",
    "        format=\"%Y-%m-%dT%H:%M:%S%.3f\",\n",
    "        strict=False   # invalid/empty → null\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80709404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_rodent_time\n",
      " status: created\n",
      "\n",
      "dim_rodent_inspection\n",
      " status: created\n",
      "\n",
      "dim_rodent_result\n",
      " status: created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# inspection dimension\n",
    "    \"job_ticket_or_work_order_id\", \"job_id\", \"job_progress\", \"inspection_date\", \"result\",\n",
    "    \"borough\", \"inspection_type\", \"zip_code\" # added attributes for analysis\n",
    "    \n",
    "    '''\n",
    "\n",
    "# creating location | no 'city' column, renaming nta to neighborhood\n",
    "dim_rodent_location = (\n",
    "    rodent_stg.select([\n",
    "    #    pl.lit(\"NY\").alias(\"state\"),\n",
    "    #    pl.lit(\"USA\").alias(\"country\"),\n",
    "        pl.col(\"nta\").alias(\"neighborhood\"),\n",
    "        \"borough\",\n",
    "        \"zip_code\"\n",
    "    ]).unique().with_row_index(\"location_dim_id\") # surrogate key\n",
    ")\n",
    "# creating parquet file\n",
    "dim_rodent_location.write_parquet(\"dim_rodent_location.parquet\")\n",
    "\n",
    "# time dimension [ created_date ] \n",
    "dim_rodent_time = (\n",
    "    rodent_stg.select([\n",
    "        pl.col(\"inspection_date\").dt.date().alias(\"date\"),\n",
    "        pl.col(\"inspection_date\").dt.year().alias(\"year\"),\n",
    "        pl.col(\"inspection_date\").dt.month().alias(\"month\"),\n",
    "        pl.col(\"inspection_date\").dt.strftime(\"%Y-%m\").alias(\"YYYY-MM\"),\n",
    "        pl.col(\"inspection_date\").dt.strftime(\"%b\").alias(\"month_name\"), # abbreviated month name\n",
    "        pl.col(\"inspection_date\").dt.day().alias(\"day\"),\n",
    "        pl.col(\"inspection_date\").dt.week().alias(\"week\"),\n",
    "        pl.col(\"inspection_date\").dt.quarter().alias(\"quarter\"),\n",
    "    ]).unique().with_row_index(\"time_dim_id\")\n",
    ")\n",
    "# creating parquet file\n",
    "dim_rodent_time.write_parquet(\"dim_rodent_time.parquet\")\n",
    "print(\"dim_rodent_time\\n\",\n",
    "      \"status: created\\n\")\n",
    "\n",
    "# inspection 'status' dimension\n",
    "dim_rodent_inspection = (\n",
    "    rodent_stg.select([\n",
    "        \"job_ticket_or_work_order_id\",\n",
    "        \"job_id\",\n",
    "        \"inspection_type\", # added, not included in documentation\n",
    "        \"job_progress\"\n",
    "    ]).unique().with_row_index(\"inspection_dim_id\")\n",
    ")\n",
    "# creating parquet file\n",
    "dim_rodent_inspection.write_parquet(\"dim_rodent_inspection.parquet\")\n",
    "print(\"dim_rodent_inspection\\n\",\n",
    "      \"status: created\\n\")\n",
    "\n",
    "# inspection dimension\n",
    "dim_rodent_result = (\n",
    "    rodent_stg.select(\n",
    "        pl.col(\"result\").alias(\"inspection_result\").unique())\n",
    "    .unique()\n",
    "    .with_row_index(\"inspection_result_dim_id\")\n",
    ")\n",
    "# creating parquet file\n",
    "dim_rodent_result.write_parquet(\"dim_rodent_result.parquet\")\n",
    "print(\"dim_rodent_result\\n\",\n",
    "      \"status: created\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0177f23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hv/4rnf1t6s623cq2d6x0n61swc0000gn/T/ipykernel_405/4108679822.py:41: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  pl.count().alias(\"inspection_count\")\n"
     ]
    }
   ],
   "source": [
    "# creating rodent inspection fact table\n",
    "\n",
    "fact_rodent = (\n",
    "    rodent_stg\n",
    "    # location join\n",
    "    .join(\n",
    "        dim_rodent_location,\n",
    "        left_on=[\"borough\", \"zip_code\", \"nta\"],\n",
    "        right_on=[\"borough\", \"zip_code\", \"neighborhood\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # time join\n",
    "    .join(\n",
    "        dim_rodent_time.select([\"time_dim_id\", \"date\"]),\n",
    "        left_on=pl.col(\"inspection_date\").dt.date(),\n",
    "        right_on=pl.col(\"date\"),\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # inspection join\n",
    "    .join(\n",
    "        dim_rodent_inspection,\n",
    "        on=[\"job_ticket_or_work_order_id\", \"job_id\", \"inspection_type\", \"job_progress\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # result join\n",
    "    .join(\n",
    "        dim_rodent_result,\n",
    "        left_on=\"result\",\n",
    "        right_on=\"inspection_result\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .group_by([\n",
    "        \"location_dim_id\",\n",
    "        \"time_dim_id\",\n",
    "        \"inspection_dim_id\",\n",
    "        \"inspection_result_dim_id\"\n",
    "    ]).agg(\n",
    "        pl.count().alias(\"inspection_count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "fact_rodent.write_parquet(\"fact_rodent_inspection.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65350ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 5)\n",
      "┌─────────────────┬─────────────┬───────────────────┬──────────────────────────┬──────────────────┐\n",
      "│ location_dim_id ┆ time_dim_id ┆ inspection_dim_id ┆ inspection_result_dim_id ┆ inspection_count │\n",
      "│ ---             ┆ ---         ┆ ---               ┆ ---                      ┆ ---              │\n",
      "│ u32             ┆ u32         ┆ u32               ┆ u32                      ┆ u32              │\n",
      "╞═════════════════╪═════════════╪═══════════════════╪══════════════════════════╪══════════════════╡\n",
      "│ 249             ┆ 802         ┆ 176277            ┆ 0                        ┆ 1                │\n",
      "│ 463             ┆ 2919        ┆ 2251705           ┆ 1                        ┆ 1                │\n",
      "│ 249             ┆ 1981        ┆ 1350765           ┆ 0                        ┆ 2                │\n",
      "│ 1061            ┆ 3204        ┆ 2357081           ┆ 5                        ┆ 1                │\n",
      "│ 977             ┆ 573         ┆ 949585            ┆ 1                        ┆ 1                │\n",
      "└─────────────────┴─────────────┴───────────────────┴──────────────────────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(fact_rodent.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ce3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
