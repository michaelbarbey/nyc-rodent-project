{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8010195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl             # faster and more efficient than pandas\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "# base_url = 'https://data.cityofnewyork.us/api/v3/views/p937-wjvj/query.csv'\n",
    "block_size = 100_000  # Number of rows per block\n",
    "#output_parquet = 'rodent_inspection_data.parquet'\n",
    "output_parquet = '311_service_requests_data.parquet'\n",
    "\n",
    "base_url = 'https://data.cityofnewyork.us/resource/erm2-nwe9.csv'\n",
    "max_workers = 4 # Number of threads for parallel processing\n",
    "\n",
    "columns = [ # channel_type --> open_data_channel_type\n",
    "    \"unique_key\", \"created_date\", \"closed_date\", \"agency\", \"agency_name\",\n",
    "    \"complaint_type\", \"descriptor\", \"location_type\", \"incident_zip\",\n",
    "    \"city\", \"status\", \"resolution_action_updated_date\", \"borough\", \"open_data_channel_type\"\n",
    "]\n",
    "\n",
    "def download_block (offset):\n",
    "    # Socrata API in SQL -- pending to test SODA 3\n",
    "    soql = (\n",
    "        f\"SELECT {', '.join(columns)} \"\n",
    "        f\"LIMIT {block_size} OFFSET {offset}\"  \n",
    "    )\n",
    "    \n",
    "    encoded_query = urllib.parse.quote(soql, safe='')\n",
    "    url = f\"{base_url}?$query={encoded_query}\"\n",
    "    \n",
    "    try: \n",
    "        df_block = pl.read_csv(url, \n",
    "                               columns=columns, \n",
    "                               schema_overrides={\"incident_zip\": pl.Utf8}\n",
    "                               )\n",
    "        if df_block.height == 0:\n",
    "            return None\n",
    "        table = df_block.to_arrow() # File type transformation to arrow\n",
    "        return (offset, table)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading block at offset {offset}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e1b9055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and wrote block at offset 0\n",
      "Downloaded and wrote block at offset 300000\n",
      "Downloaded and wrote block at offset 200000\n",
      "Downloaded and wrote block at offset 100000\n",
      "Downloaded and wrote block at offset 500000\n",
      "Downloaded and wrote block at offset 400000\n",
      "Downloaded and wrote block at offset 600000\n",
      "Downloaded and wrote block at offset 700000\n",
      "Downloaded and wrote block at offset 1000000\n",
      "Downloaded and wrote block at offset 900000\n",
      "Downloaded and wrote block at offset 1100000\n",
      "Downloaded and wrote block at offset 800000\n",
      "Downloaded and wrote block at offset 1500000\n",
      "Downloaded and wrote block at offset 1200000\n",
      "Downloaded and wrote block at offset 1400000\n",
      "Downloaded and wrote block at offset 1300000\n",
      "Downloaded and wrote block at offset 1600000\n",
      "Downloaded and wrote block at offset 1800000\n",
      "Downloaded and wrote block at offset 1900000\n",
      "Downloaded and wrote block at offset 1700000\n",
      "Downloaded and wrote block at offset 2200000\n",
      "Downloaded and wrote block at offset 2000000\n",
      "Downloaded and wrote block at offset 2100000\n",
      "Downloaded and wrote block at offset 2300000\n",
      "Downloaded and wrote block at offset 2400000\n",
      "Downloaded and wrote block at offset 2500000\n",
      "Downloaded and wrote block at offset 2700000\n",
      "Downloaded and wrote block at offset 2600000\n",
      "Downloaded and wrote block at offset 2800000\n",
      "Downloaded and wrote block at offset 2900000\n",
      "Downloaded and wrote block at offset 3000000\n",
      "Downloaded and wrote block at offset 3100000\n",
      "Downloaded and wrote block at offset 3300000\n",
      "Downloaded and wrote block at offset 3200000\n",
      "Downloaded and wrote block at offset 3400000\n",
      "Downloaded and wrote block at offset 3500000\n",
      "Downloaded and wrote block at offset 3600000\n",
      "Downloaded and wrote block at offset 3900000\n",
      "Downloaded and wrote block at offset 3700000\n",
      "Downloaded and wrote block at offset 3800000\n",
      "Downloaded and wrote block at offset 4100000\n",
      "Downloaded and wrote block at offset 4200000\n",
      "Downloaded and wrote block at offset 4300000\n",
      "Downloaded and wrote block at offset 4000000\n",
      "Downloaded and wrote block at offset 4600000\n",
      "Downloaded and wrote block at offset 4500000\n",
      "Downloaded and wrote block at offset 4700000\n",
      "Downloaded and wrote block at offset 4400000\n",
      "Downloaded and wrote block at offset 4900000\n",
      "Downloaded and wrote block at offset 5000000\n",
      "Downloaded and wrote block at offset 4800000\n",
      "Downloaded and wrote block at offset 5100000\n",
      "Downloaded and wrote block at offset 5500000\n",
      "Downloaded and wrote block at offset 5400000\n",
      "Downloaded and wrote block at offset 5200000\n",
      "Downloaded and wrote block at offset 5300000\n",
      "Downloaded and wrote block at offset 5600000\n",
      "Downloaded and wrote block at offset 5900000\n",
      "Downloaded and wrote block at offset 5800000\n",
      "Downloaded and wrote block at offset 5700000\n",
      "Downloaded and wrote block at offset 6000000\n",
      "Downloaded and wrote block at offset 6100000\n",
      "Downloaded and wrote block at offset 6300000\n",
      "Downloaded and wrote block at offset 6200000\n",
      "Downloaded and wrote block at offset 6600000\n",
      "Downloaded and wrote block at offset 6500000\n",
      "Downloaded and wrote block at offset 6700000\n",
      "Downloaded and wrote block at offset 6400000\n",
      "Downloaded and wrote block at offset 7100000\n",
      "Downloaded and wrote block at offset 7000000\n",
      "Downloaded and wrote block at offset 6900000\n",
      "Downloaded and wrote block at offset 6800000\n",
      "Downloaded and wrote block at offset 7500000\n",
      "Downloaded and wrote block at offset 7400000\n",
      "Downloaded and wrote block at offset 7300000\n",
      "Downloaded and wrote block at offset 7200000\n",
      "Downloaded and wrote block at offset 7800000\n",
      "Downloaded and wrote block at offset 7700000\n",
      "Downloaded and wrote block at offset 7900000\n",
      "Downloaded and wrote block at offset 7600000\n",
      "Downloaded and wrote block at offset 8000000\n",
      "Downloaded and wrote block at offset 8300000\n",
      "Downloaded and wrote block at offset 8200000\n",
      "Downloaded and wrote block at offset 8100000\n",
      "Downloaded and wrote block at offset 8600000\n",
      "Downloaded and wrote block at offset 8400000\n",
      "Downloaded and wrote block at offset 8700000\n",
      "Downloaded and wrote block at offset 8500000\n",
      "Downloaded and wrote block at offset 8800000\n",
      "Downloaded and wrote block at offset 9000000\n",
      "Downloaded and wrote block at offset 8900000\n",
      "Downloaded and wrote block at offset 9100000\n",
      "Downloaded and wrote block at offset 9400000\n",
      "Downloaded and wrote block at offset 9200000\n",
      "Downloaded and wrote block at offset 9300000\n",
      "Downloaded and wrote block at offset 9500000\n",
      "Downloaded and wrote block at offset 9600000\n",
      "Downloaded and wrote block at offset 9700000\n",
      "Downloaded and wrote block at offset 9900000\n",
      "Downloaded and wrote block at offset 9800000\n",
      "Downloaded and wrote block at offset 10100000\n",
      "Downloaded and wrote block at offset 10000000\n",
      "Downloaded and wrote block at offset 10200000\n",
      "Downloaded and wrote block at offset 10300000\n",
      "Error downloading block at offset 10600000: <urlopen error [Errno 8] nodename nor servname provided, or not known>\n",
      "Downloaded and wrote block at offset 10500000\n",
      "Downloaded and wrote block at offset 10700000\n",
      "Downloaded and wrote block at offset 10400000\n",
      "All data downloaded and saved to Parquet.\n"
     ]
    }
   ],
   "source": [
    "# download loop and offset counter\n",
    "\n",
    "offset = 0\n",
    "first_block = True\n",
    "writer = None\n",
    "\n",
    "while True:\n",
    "    # Use ThreadPoolExecutor to download multiple blocks in parallel\n",
    "    offsets = [offset + i*block_size for i in range(max_workers)]\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(download_block, o): o for o in offsets}\n",
    "        \n",
    "        all_done = False\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is None:\n",
    "                all_done = True\n",
    "                continue\n",
    "            \n",
    "            block_offset, table = result\n",
    "            \n",
    "            # Initialize ParquetWriter on first block\n",
    "            if first_block:\n",
    "                writer = pq.ParquetWriter('311_service_requests_data.parquet', table.schema, compression='snappy')\n",
    "                first_block = False\n",
    "            \n",
    "            writer.write_table(table)\n",
    "            print(f\"Downloaded and wrote block at offset {block_offset}\")\n",
    "    if all_done:\n",
    "        break\n",
    "    offset += max_workers * block_size\n",
    "    \n",
    "# Closing file writer\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "print(\"All data downloaded and saved to Parquet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ba9c06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building dimensional models --> modified application, create single build\n",
    "\n",
    "import urllib.parse\n",
    "base_url = 'https://data.cityofnewyork.us/resource/p937-wjvj.csv'\n",
    "block_size = 100_000  # Number of rows per block\n",
    "output_parquet = 'rodent_inspection_data.parquet'\n",
    "#output_parquet = '311_service_requests_data.parquet'\n",
    "\n",
    "#base_url = 'https://data.cityofnewyork.us/resource/erm2-nwe9.csv'\n",
    "max_workers = 4 # Number of threads for parallel processing\n",
    "\n",
    "columns = [ # inspection dimension\n",
    "    \"job_ticket_or_work_order_id\", \"job_id\", \"job_progress\", \"inspection_date\", \"result\",\n",
    "    \"borough\", \"inspection_type\", \"zip_code\", \"nta\" # added attributes for analysis\n",
    "]\n",
    "\n",
    "def download_block (offset):\n",
    "    # Socrata API in SQL -- pending to test SODA 3\n",
    "    soql = (\n",
    "        f\"SELECT {', '.join(columns)} \"\n",
    "        f\"LIMIT {block_size} OFFSET {offset}\"  \n",
    "    )\n",
    "    \n",
    "    encoded_query = urllib.parse.quote(soql, safe='')\n",
    "    url = f\"{base_url}?$query={encoded_query}\"\n",
    "    \n",
    "    try: \n",
    "        df_block = pl.read_csv(url, \n",
    "                               columns=columns, \n",
    "                               schema_overrides={\"zip_code\": pl.Utf8}\n",
    "                               )\n",
    "        if df_block.height == 0:\n",
    "            return None\n",
    "        table = df_block.to_arrow() # File type transformation to arrow\n",
    "        return (offset, table)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading block at offset {offset}: {e}\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b4fe9e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and wrote block at offset 100000\n",
      "Downloaded and wrote block at offset 0\n",
      "Downloaded and wrote block at offset 300000\n",
      "Downloaded and wrote block at offset 200000\n",
      "Downloaded and wrote block at offset 500000\n",
      "Downloaded and wrote block at offset 400000\n",
      "Downloaded and wrote block at offset 600000\n",
      "Downloaded and wrote block at offset 700000\n",
      "Downloaded and wrote block at offset 900000\n",
      "Downloaded and wrote block at offset 1100000\n",
      "Downloaded and wrote block at offset 800000\n",
      "Downloaded and wrote block at offset 1000000\n",
      "Downloaded and wrote block at offset 1200000\n",
      "Downloaded and wrote block at offset 1400000\n",
      "Downloaded and wrote block at offset 1300000\n",
      "Downloaded and wrote block at offset 1500000\n",
      "Downloaded and wrote block at offset 1700000\n",
      "Downloaded and wrote block at offset 1600000\n",
      "Downloaded and wrote block at offset 1800000\n",
      "Downloaded and wrote block at offset 1900000\n",
      "Downloaded and wrote block at offset 2300000\n",
      "Downloaded and wrote block at offset 2000000\n",
      "Downloaded and wrote block at offset 2100000\n",
      "Downloaded and wrote block at offset 2200000\n",
      "Downloaded and wrote block at offset 2400000\n",
      "Downloaded and wrote block at offset 2500000\n",
      "Downloaded and wrote block at offset 2700000\n",
      "Downloaded and wrote block at offset 2600000\n",
      "Downloaded and wrote block at offset 2800000\n",
      "All data downloaded and saved to Parquet.\n"
     ]
    }
   ],
   "source": [
    "# download loop and offset counter\n",
    "\n",
    "offset = 0\n",
    "first_block = True\n",
    "writer = None\n",
    "\n",
    "while True:\n",
    "    # Use ThreadPoolExecutor to download multiple blocks in parallel\n",
    "    offsets = [offset + i*block_size for i in range(max_workers)]\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(download_block, o): o for o in offsets}\n",
    "        \n",
    "        all_done = False\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is None:\n",
    "                all_done = True\n",
    "                continue\n",
    "            \n",
    "            block_offset, table = result\n",
    "            \n",
    "            # Initialize ParquetWriter on first block\n",
    "            if first_block:\n",
    "                writer = pq.ParquetWriter('rodent_inspection_data.parquet', table.schema, compression='snappy')\n",
    "                first_block = False\n",
    "            \n",
    "            writer.write_table(table)\n",
    "            print(f\"Downloaded and wrote block at offset {block_offset}\")\n",
    "    if all_done:\n",
    "        break\n",
    "    offset += max_workers * block_size\n",
    "    \n",
    "# Closing file writer\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "print(\"All data downloaded and saved to Parquet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d35f6b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "stg = pl.read_parquet(\"311_service_requests_data.parquet\")\n",
    "\n",
    "# Convert created_date from string → datetime (ISO-ish: 2025-03-31T17:43:18.000)\n",
    "stg = stg.with_columns(\n",
    "    pl.col(\"created_date\").str.strptime(\n",
    "        pl.Datetime,\n",
    "        format=\"%Y-%m-%dT%H:%M:%S%.3f\",\n",
    "        strict=False   # invalid/empty → null\n",
    "    )\n",
    ")\n",
    "\n",
    "# converting closed_date and resolution_action_updated_date to datetime\n",
    "stg = stg.with_columns(\n",
    "    pl.col(\"closed_date\").str.strptime(\n",
    "        pl.Datetime,\n",
    "        format=\"%Y-%m-%dT%H:%M:%S%.3f\",\n",
    "        strict=False\n",
    "    ),\n",
    "    pl.col(\"resolution_action_updated_date\").str.strptime(\n",
    "        pl.Datetime,\n",
    "        format=\"%Y-%m-%dT%H:%M:%S%.3f\",\n",
    "        strict=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f622af45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_time\n",
      " status: created\n",
      "\n",
      "dim_agency\n",
      " status: created\n",
      "\n",
      "dim_status\n",
      " status: created\n",
      "\n",
      "dim_channel\n",
      " status: created\n",
      "\n",
      "dim_complaint\n",
      " status: created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# location dimension build | select query\n",
    "dim_location = (\n",
    "    stg.select([\n",
    "        pl.lit(\"NY\").alias(\"state\"),\n",
    "        pl.lit(\"USA\").alias(\"country\"),\n",
    "        \"city\",\n",
    "        \"borough\",\n",
    "        pl.col(\"incident_zip\").alias(\"zip_code\")\n",
    "    ]).unique().with_row_index(\"location_dim_id\") # surrogate key\n",
    ")\n",
    "# creating parquet file\n",
    "dim_location.write_parquet(\"dim_location.parquet\")\n",
    "\n",
    "# time dimension [ created_date ] \n",
    "dim_time = (\n",
    "    stg.select([\n",
    "        pl.col(\"created_date\").dt.date().alias(\"date\"),\n",
    "        pl.col(\"created_date\").dt.year().alias(\"year\"),\n",
    "        pl.col(\"created_date\").dt.month().alias(\"month\"),\n",
    "        pl.col(\"created_date\").dt.strftime(\"%Y-%m\").alias(\"YYYY-MM\"),\n",
    "        pl.col(\"created_date\").dt.strftime(\"%b\").alias(\"month_name\"), # abbreviated month name\n",
    "        pl.col(\"created_date\").dt.day().alias(\"day\"),\n",
    "        pl.col(\"created_date\").dt.week().alias(\"week\"),\n",
    "        pl.col(\"created_date\").dt.quarter().alias(\"quarter\"),\n",
    "    ]).unique().with_row_index(\"time_dim_id\")\n",
    ")\n",
    "# creating parquet file\n",
    "dim_time.write_parquet(\"dim_time.parquet\")\n",
    "print(\"dim_time\\n\",\n",
    "      \"status: created\\n\")\n",
    "\n",
    "# agency dimension\n",
    "dim_agency = (\n",
    "    stg.select([\n",
    "        \"agency\",\n",
    "        \"agency_name\"\n",
    "    ]).unique().with_row_index(\"agency_dim_id\")\n",
    ")\n",
    "# creating parquet file\n",
    "dim_agency.write_parquet(\"dim_agency.parquet\")\n",
    "print(\"dim_agency\\n\",\n",
    "      \"status: created\\n\")\n",
    "\n",
    "# status dimension\n",
    "dim_status = (\n",
    "    stg.select(pl.col(\"status\").alias(\"status_type\"))\n",
    "    .unique().with_row_index(\"status_dim_id\")\n",
    ")\n",
    "# creating parquet file\n",
    "dim_status.write_parquet(\"dim_status.parquet\")\n",
    "print(\"dim_status\\n\",\n",
    "      \"status: created\\n\")\n",
    "\n",
    "# channel type dimension\n",
    "dim_channel = (\n",
    "    stg.select(pl.col(\"open_data_channel_type\").alias(\"channel_type\"))\n",
    "    .unique().with_row_index(\"channel_type_dim_id\")\n",
    ")\n",
    "# creating parquet file\n",
    "dim_channel.write_parquet(\"dim_channel_type.parquet\")\n",
    "print(\"dim_channel\\n\",\n",
    "      \"status: created\\n\")\n",
    "\n",
    "# complaint type dimension\n",
    "dim_complaint = (\n",
    "    stg.select(pl.col(\"complaint_type\").alias(\"complaint_name\"))\n",
    "    .unique().with_row_index(\"complaint_type_dim_id\")\n",
    ")\n",
    "# creating parquet file\n",
    "dim_complaint.write_parquet(\"dim_complaint_type.parquet\")\n",
    "print(\"dim_complaint\\n\",\n",
    "      \"status: created\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a241b4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10, 1)\n",
      "┌─────────────────────┐\n",
      "│ created_date        │\n",
      "│ ---                 │\n",
      "│ datetime[ms]        │\n",
      "╞═════════════════════╡\n",
      "│ 2025-03-31 17:43:18 │\n",
      "│ 2025-03-31 20:02:25 │\n",
      "│ 2025-10-23 14:16:09 │\n",
      "│ 2025-10-23 13:49:01 │\n",
      "│ 2025-10-23 07:25:38 │\n",
      "│ 2025-11-21 12:45:22 │\n",
      "│ 2025-10-23 11:33:00 │\n",
      "│ 2025-04-01 11:46:52 │\n",
      "│ 2025-06-05 15:58:18 │\n",
      "│ 2025-04-01 11:46:52 │\n",
      "└─────────────────────┘\n",
      "Schema({'unique_key': Int64, 'created_date': Datetime(time_unit='ms', time_zone=None), 'closed_date': Datetime(time_unit='ms', time_zone=None), 'agency': String, 'agency_name': String, 'complaint_type': String, 'descriptor': String, 'location_type': String, 'incident_zip': String, 'city': String, 'status': String, 'resolution_action_updated_date': Datetime(time_unit='ms', time_zone=None), 'borough': String, 'open_data_channel_type': String})\n"
     ]
    }
   ],
   "source": [
    "print(stg.select(pl.col(\"created_date\").tail()))\n",
    "print(stg.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b54047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating 311 service requests fact table\n",
    "fact = (\n",
    "    stg\n",
    "    # location join\n",
    "    .join(dim_location, left_on=[\"city\", \"borough\", \"incident_zip\"], \n",
    "                        right_on=[\"city\", \"borough\", \"zip_code\"], how=\"left\")\n",
    "    \n",
    "    # time join\n",
    "    .join(dim_time.select([\"time_dim_id\", \"date\"]), left_on=pl.col(\"created_date\").dt.date(), right_on=pl.col(\"date\"),how=\"left\")\n",
    "    \n",
    "    # agency join\n",
    "    .join(dim_agency, on=[\"agency\", \"agency_name\"], how=\"left\")\n",
    "    \n",
    "    # status join\n",
    "    .join(dim_status, left_on=\"status\", right_on=\"status_type\", how=\"left\")\n",
    "    \n",
    "    # channel type join\n",
    "    .join(dim_channel, left_on=\"open_data_channel_type\", right_on=\"channel_type\", how=\"left\")\n",
    "    \n",
    "    # complaint type join\n",
    "    .join(dim_complaint, left_on=\"complaint_type\", right_on=\"complaint_name\", how=\"left\")\n",
    "    \n",
    "    # aggregate to match the fact grain\n",
    "    .group_by([\n",
    "        \"location_dim_id\",\n",
    "        \"time_dim_id\",\n",
    "        \"channel_type_dim_id\",\n",
    "        \"complaint_type_dim_id\",\n",
    "        \"agency_dim_id\",\n",
    "        \"status_dim_id\"\n",
    "    ]).agg(\n",
    "        pl.count(\"unique_key\").alias(\"complaint_count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "fact.write_parquet(\"fact_311_service_requests.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc2e7805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 7)\n",
      "┌──────────────┬─────────────┬─────────────┬─────────────┬─────────────┬─────────────┬─────────────┐\n",
      "│ location_dim ┆ time_dim_id ┆ channel_typ ┆ complaint_t ┆ agency_dim_ ┆ status_dim_ ┆ complaint_c │\n",
      "│ _id          ┆ ---         ┆ e_dim_id    ┆ ype_dim_id  ┆ id          ┆ id          ┆ ount        │\n",
      "│ ---          ┆ u32         ┆ ---         ┆ ---         ┆ ---         ┆ ---         ┆ ---         │\n",
      "│ u32          ┆             ┆ u32         ┆ u32         ┆ u32         ┆ u32         ┆ u32         │\n",
      "╞══════════════╪═════════════╪═════════════╪═════════════╪═════════════╪═════════════╪═════════════╡\n",
      "│ 2612         ┆ 4161        ┆ 3           ┆ 45          ┆ 1018        ┆ 4           ┆ 1           │\n",
      "│ 494          ┆ 1862        ┆ 2           ┆ 282         ┆ 529         ┆ 4           ┆ 4           │\n",
      "│ 2590         ┆ 2552        ┆ 4           ┆ 220         ┆ 445         ┆ 4           ┆ 2           │\n",
      "│ 2342         ┆ 3395        ┆ 2           ┆ 124         ┆ 529         ┆ 4           ┆ 1           │\n",
      "│ 2811         ┆ 4837        ┆ 2           ┆ 13          ┆ 355         ┆ 4           ┆ 2           │\n",
      "└──────────────┴─────────────┴─────────────┴─────────────┴─────────────┴─────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(fact.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10325dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating rodent inspection data dimensional model\n",
    "\n",
    "rodent_stg = pl.read_parquet(\"rodent_inspection_data.parquet\")\n",
    "\n",
    "# Convert created_date from string → datetime (ISO-ish: 2025-03-31T17:43:18.000)\n",
    "rodent_stg = rodent_stg.with_columns(\n",
    "    pl.col(\"inspection_date\").str.strptime(\n",
    "        pl.Datetime,\n",
    "        format=\"%Y-%m-%dT%H:%M:%S%.3f\",\n",
    "        strict=False   # invalid/empty → null\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "80709404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_rodent_time\n",
      " status: created\n",
      "\n",
      "dim_rodent_inspection\n",
      " status: created\n",
      "\n",
      "dim_rodent_result\n",
      " status: created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# inspection dimension\n",
    "    \"job_ticket_or_work_order_id\", \"job_id\", \"job_progress\", \"inspection_date\", \"result\",\n",
    "    \"borough\", \"inspection_type\", \"zip_code\" # added attributes for analysis\n",
    "    \n",
    "    '''\n",
    "\n",
    "# creating location | no 'city' column, renaming nta to neighborhood\n",
    "dim_rodent_location = (\n",
    "    rodent_stg.select([\n",
    "    #    pl.lit(\"NY\").alias(\"state\"),\n",
    "    #    pl.lit(\"USA\").alias(\"country\"),\n",
    "        pl.col(\"nta\").alias(\"neighborhood\"),\n",
    "        \"borough\",\n",
    "        \"zip_code\"\n",
    "    ]).unique().with_row_index(\"location_dim_id\") # surrogate key\n",
    ")\n",
    "# creating parquet file\n",
    "dim_rodent_location.write_parquet(\"dim_rodent_location.parquet\")\n",
    "\n",
    "# time dimension [ created_date ] \n",
    "dim_rodent_time = (\n",
    "    rodent_stg.select([\n",
    "        pl.col(\"inspection_date\").dt.date().alias(\"date\"),\n",
    "        pl.col(\"inspection_date\").dt.year().alias(\"year\"),\n",
    "        pl.col(\"inspection_date\").dt.month().alias(\"month\"),\n",
    "        pl.col(\"inspection_date\").dt.strftime(\"%Y-%m\").alias(\"YYYY-MM\"),\n",
    "        pl.col(\"inspection_date\").dt.strftime(\"%b\").alias(\"month_name\"), # abbreviated month name\n",
    "        pl.col(\"inspection_date\").dt.day().alias(\"day\"),\n",
    "        pl.col(\"inspection_date\").dt.week().alias(\"week\"),\n",
    "        pl.col(\"inspection_date\").dt.quarter().alias(\"quarter\"),\n",
    "    ]).unique().with_row_index(\"time_dim_id\")\n",
    ")\n",
    "# creating parquet file\n",
    "dim_rodent_time.write_parquet(\"dim_rodent_time.parquet\")\n",
    "print(\"dim_rodent_time\\n\",\n",
    "      \"status: created\\n\")\n",
    "\n",
    "# inspection 'status' dimension\n",
    "dim_rodent_inspection = (\n",
    "    rodent_stg.select([\n",
    "        \"job_ticket_or_work_order_id\",\n",
    "        \"job_id\",\n",
    "        \"inspection_type\", # added, not included in documentation\n",
    "        \"job_progress\"\n",
    "    ]).unique().with_row_index(\"inspection_dim_id\")\n",
    ")\n",
    "# creating parquet file\n",
    "dim_rodent_inspection.write_parquet(\"dim_rodent_inspection.parquet\")\n",
    "print(\"dim_rodent_inspection\\n\",\n",
    "      \"status: created\\n\")\n",
    "\n",
    "# inspection dimension\n",
    "dim_rodent_result = (\n",
    "    rodent_stg.select(\n",
    "        pl.col(\"result\").alias(\"inspection_result\").unique())\n",
    "    .unique()\n",
    "    .with_row_index(\"inspection_result_dim_id\")\n",
    ")\n",
    "# creating parquet file\n",
    "dim_rodent_result.write_parquet(\"dim_rodent_result.parquet\")\n",
    "print(\"dim_rodent_result\\n\",\n",
    "      \"status: created\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0177f23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hv/4rnf1t6s623cq2d6x0n61swc0000gn/T/ipykernel_58254/4108679822.py:41: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  pl.count().alias(\"inspection_count\")\n"
     ]
    }
   ],
   "source": [
    "# creating rodent inspection fact table\n",
    "\n",
    "fact_rodent = (\n",
    "    rodent_stg\n",
    "    # location join\n",
    "    .join(\n",
    "        dim_rodent_location,\n",
    "        left_on=[\"borough\", \"zip_code\", \"nta\"],\n",
    "        right_on=[\"borough\", \"zip_code\", \"neighborhood\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # time join\n",
    "    .join(\n",
    "        dim_rodent_time.select([\"time_dim_id\", \"date\"]),\n",
    "        left_on=pl.col(\"inspection_date\").dt.date(),\n",
    "        right_on=pl.col(\"date\"),\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # inspection join\n",
    "    .join(\n",
    "        dim_rodent_inspection,\n",
    "        on=[\"job_ticket_or_work_order_id\", \"job_id\", \"inspection_type\", \"job_progress\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # result join\n",
    "    .join(\n",
    "        dim_rodent_result,\n",
    "        left_on=\"result\",\n",
    "        right_on=\"inspection_result\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .group_by([\n",
    "        \"location_dim_id\",\n",
    "        \"time_dim_id\",\n",
    "        \"inspection_dim_id\",\n",
    "        \"inspection_result_dim_id\"\n",
    "    ]).agg(\n",
    "        pl.count().alias(\"inspection_count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "fact_rodent.write_parquet(\"fact_rodent_inspection.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65350ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 5)\n",
      "┌─────────────────┬─────────────┬───────────────────┬──────────────────────────┬──────────────────┐\n",
      "│ location_dim_id ┆ time_dim_id ┆ inspection_dim_id ┆ inspection_result_dim_id ┆ inspection_count │\n",
      "│ ---             ┆ ---         ┆ ---               ┆ ---                      ┆ ---              │\n",
      "│ u32             ┆ u32         ┆ u32               ┆ u32                      ┆ u32              │\n",
      "╞═════════════════╪═════════════╪═══════════════════╪══════════════════════════╪══════════════════╡\n",
      "│ 924             ┆ 4760        ┆ 238295            ┆ 4                        ┆ 1                │\n",
      "│ 503             ┆ 2683        ┆ 2341854           ┆ 5                        ┆ 1                │\n",
      "│ 85              ┆ 3572        ┆ 1272879           ┆ 4                        ┆ 1                │\n",
      "│ 112             ┆ 902         ┆ 1115647           ┆ 5                        ┆ 1                │\n",
      "│ 368             ┆ 4423        ┆ 502097            ┆ 5                        ┆ 1                │\n",
      "└─────────────────┴─────────────┴───────────────────┴──────────────────────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(fact_rodent.tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
